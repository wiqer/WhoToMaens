# Python Dependency and Docker Deployment Guide

## 1. CUDA and PyTorch Configuration

### CUDA Version Selection
- Base Image: `nvcr.io/nvidia/cuda:12.2.2-devel-ubuntu22.04`
- CUDA Version: 12.2.2
- cuDNN Version: 8.9.0
- TensorRT Version: 10.4.0

### Environment Variables
```bash
ENV CUDA_HOME=/usr/local/cuda-12.2
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}
ENV ORT_CUDA_VERSION=12.2
ENV CUDA_VERSION=12.2.2
ENV CUDNN_VERSION=8.9.0
```

## 2. Python Dependencies Management

### Core Dependencies
```txt
# Core dependencies
streamlit==1.45.1
python-jose[cryptography]==3.5.0
bcrypt==4.3.0
cryptography==41.0.7
loguru==0.7.3
sqlalchemy==2.0.41
psycopg2-binary==2.9.10
python-dotenv==1.1.0
pyyaml==6.0.2
pydantic==2.11.5
```

### PyTorch and Deep Learning
```txt
# PyTorch with CUDA 12.2
--extra-index-url https://download.pytorch.org/whl/cu122
torch==2.5.0+cu122
torchvision==0.22.1
torchaudio==2.6.0

# Deep Learning Frameworks
transformers==4.36.2
sentence-transformers==2.2.2
peft==0.7.1
accelerate==0.27.2
bitsandbytes==0.41.1
safetensors==0.5.3
```

### GPU Acceleration Libraries
```txt
# GPU Acceleration
cupy-cuda12x==13.4.0
tensorrt==10.4.0
onnxruntime-gpu>=1.17.0
```

## 3. Docker Configuration

### Base Dockerfile Structure
```dockerfile
FROM nvcr.io/nvidia/cuda:12.2.2-devel-ubuntu22.04

# Set pip source
ARG PIP_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple/
ENV PIP_INDEX_URL=${PIP_INDEX_URL}

# Set environment variables
ENV PATH="/opt/miniconda/bin:$PATH"
ENV PYTHONPATH="/bugagaric"
ENV NVIDIA_VISIBLE_DEVICES=all
ENV NVIDIA_DRIVER_CAPABILITIES=compute,utility

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    gcc \
    g++ \
    make \
    wget \
    git \
    curl \
    ca-certificates \
    libpq-dev \
    python3-dev \
    software-properties-common \
    cuda-cudart-12-2 \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev

# Install cuDNN
RUN apt-get update && apt-get install -y --no-install-recommends \
    libcudnn8=${CUDNN_VERSION}* \
    libcudnn8-dev=${CUDNN_VERSION}*
```

### Docker Compose Configuration
```yaml
services:
  webui:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - .:/bugagaric
      - pip-cache:/root/.cache/pip
    environment:
      - PYTHONPATH=/bugagaric
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
              options:
                nvidia.com.cuda: "12.2"
    shm_size: '8gb'
```

## 4. Common Issues and Solutions

### 1. CUDA Version Compatibility
- Always verify CUDA version compatibility with PyTorch
- Use the correct PyTorch wheel for your CUDA version
- Check NVIDIA driver compatibility

### 2. Dependency Conflicts
- Use specific versions for critical packages
- Install PyTorch first, then other dependencies
- Use `--no-deps` flag when necessary for specific packages

### 3. Memory Issues
- Set appropriate `shm_size` in Docker
- Configure proper ulimits for memory
- Monitor GPU memory usage

### 4. Build Performance
- Use multi-stage builds
- Leverage Docker layer caching
- Use appropriate pip mirrors for faster downloads

## 5. Best Practices

1. **Version Pinning**
   - Always pin specific versions for critical packages
   - Document version compatibility matrix
   - Test with exact versions before deployment

2. **Docker Optimization**
   - Use multi-stage builds
   - Minimize layer count
   - Clean up unnecessary files in the same layer

3. **Security**
   - Use non-root users in containers
   - Scan dependencies for vulnerabilities
   - Keep base images updated

4. **Performance**
   - Use appropriate CUDA versions
   - Configure proper GPU memory allocation
   - Optimize Python package installation order

## 6. Troubleshooting Guide

### Common Errors and Solutions

1. **CUDA Version Mismatch**
   ```bash
   # Check CUDA version
   nvcc --version
   # Check PyTorch CUDA availability
   python -c "import torch; print(torch.cuda.is_available())"
   ```

2. **Memory Issues**
   ```bash
   # Check GPU memory
   nvidia-smi
   # Monitor memory usage
   watch -n 1 nvidia-smi
   ```

3. **Dependency Conflicts**
   ```bash
   # Create fresh environment
   conda create -n new_env python=3.10
   # Install dependencies in correct order
   pip install torch==2.5.0+cu122
   pip install -r requirements.txt
   ```

## 7. References

- [NVIDIA CUDA Documentation](https://docs.nvidia.com/cuda/)
- [PyTorch Installation Guide](https://pytorch.org/get-started/locally/)
- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)
- [Python Package Index](https://pypi.org/)