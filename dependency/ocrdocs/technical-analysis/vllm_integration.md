# MaoOCR vLLMé›†æˆåˆ†æ

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æå¦‚ä½•åœ¨MaoOCRä¸­é›†æˆvLLMæ¡†æ¶ï¼Œå®ç°æœ¬åœ°å¤§æ¨¡å‹æ¨ç†å’Œå¤šæ¥å£é£æ ¼APIæ”¯æŒã€‚

## ğŸ—ï¸ vLLMæ¡†æ¶åˆ†æ

### vLLMä¼˜åŠ¿

#### 1. é«˜æ€§èƒ½æ¨ç†
- **PagedAttention**: é«˜æ•ˆçš„å†…å­˜ç®¡ç†æœºåˆ¶
- **è¿ç»­æ‰¹å¤„ç†**: æ”¯æŒåŠ¨æ€æ‰¹å¤„ç†å¤§å°
- **GPUå†…å­˜ä¼˜åŒ–**: å‡å°‘å†…å­˜ç¢ç‰‡ï¼Œæé«˜åˆ©ç”¨ç‡
- **å¹¶è¡Œæ¨ç†**: æ”¯æŒå¤šGPUå¹¶è¡Œæ¨ç†

#### 2. æ¨¡å‹æ”¯æŒ
- **Hugging Faceæ¨¡å‹**: æ”¯æŒæ‰€æœ‰Hugging Faceæ ¼å¼æ¨¡å‹
- **è‡ªå®šä¹‰æ¨¡å‹**: æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹æ¶æ„
- **é‡åŒ–æ”¯æŒ**: æ”¯æŒINT4/INT8é‡åŒ–
- **LoRAæ”¯æŒ**: æ”¯æŒLoRAå¾®è°ƒæ¨¡å‹

#### 3. APIæ¥å£
- **OpenAIå…¼å®¹**: æ”¯æŒOpenAIé£æ ¼API
- **RESTful API**: æ ‡å‡†HTTPæ¥å£
- **WebSocket**: å®æ—¶æµå¼è¾“å‡º
- **gRPC**: é«˜æ€§èƒ½RPCæ¥å£

### åœ¨MaoOCRä¸­çš„åº”ç”¨åœºæ™¯

#### 1. å›¾åƒå¤æ‚åº¦åˆ†æ
```python
# ä½¿ç”¨vLLMè¿è¡Œè§†è§‰è¯­è¨€æ¨¡å‹åˆ†æå›¾åƒå¤æ‚åº¦
class VLLMComplexityAnalyzer:
    def __init__(self, model_path: str):
        self.llm = vllm.LLM(model=model_path)
        self.engine = vllm.AsyncLLMEngine.from_llm(self.llm)
    
    async def analyze_complexity(self, image_path: str) -> Dict[str, Any]:
        """åˆ†æå›¾åƒå¤æ‚åº¦"""
        # æ„å»ºæç¤ºè¯
        prompt = self._build_complexity_prompt(image_path)
        
        # ä½¿ç”¨vLLMæ¨ç†
        sampling_params = vllm.SamplingParams(
            temperature=0.1,
            max_tokens=512,
            stop=["\n\n"]
        )
        
        results = await self.engine.generate([prompt], sampling_params)
        return self._parse_complexity_result(results[0].outputs[0].text)
```

#### 2. ç­–ç•¥é€‰æ‹©ä¼˜åŒ–
```python
# ä½¿ç”¨vLLMä¼˜åŒ–ç­–ç•¥é€‰æ‹©
class VLLMStrategySelector:
    def __init__(self, model_path: str):
        self.llm = vllm.LLM(model=model_path)
    
    async def select_optimal_strategy(self, 
                                    image_info: Dict[str, Any],
                                    resource_status: Dict[str, Any]) -> OCRStrategy:
        """é€‰æ‹©æœ€ä¼˜OCRç­–ç•¥"""
        prompt = self._build_strategy_prompt(image_info, resource_status)
        
        sampling_params = vllm.SamplingParams(
            temperature=0.2,
            max_tokens=256,
            stop=["\n"]
        )
        
        results = self.llm.generate([prompt], sampling_params)
        strategy_text = results[0].outputs[0].text
        
        return self._parse_strategy(strategy_text)
```

#### 3. å¤šæ¨¡æ€æ–‡æ¡£ç†è§£
```python
# ä½¿ç”¨vLLMè¿›è¡Œå¤šæ¨¡æ€æ–‡æ¡£ç†è§£
class VLLMMultimodalProcessor:
    def __init__(self, model_path: str):
        self.llm = vllm.LLM(model=model_path)
    
    async def process_document(self, 
                             image_path: str, 
                             text_content: str) -> DocumentAnalysis:
        """å¤„ç†å¤šæ¨¡æ€æ–‡æ¡£"""
        prompt = self._build_multimodal_prompt(image_path, text_content)
        
        sampling_params = vllm.SamplingParams(
            temperature=0.1,
            max_tokens=1024
        )
        
        results = self.llm.generate([prompt], sampling_params)
        analysis_text = results[0].outputs[0].text
        
        return self._parse_document_analysis(analysis_text)
```

## ğŸ”§ æŠ€æœ¯å®ç°

### 1. vLLMæœåŠ¡é›†æˆ

#### æœ¬åœ°vLLMæœåŠ¡
```python
# vLLMæœåŠ¡ç®¡ç†å™¨
class VLLMServiceManager:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.models = {}
        self.engines = {}
    
    async def start_service(self, model_name: str, model_path: str):
        """å¯åŠ¨vLLMæœåŠ¡"""
        try:
            # åˆ›å»ºLLMå®ä¾‹
            llm = vllm.LLM(
                model=model_path,
                tensor_parallel_size=self.config.get('tensor_parallel_size', 1),
                gpu_memory_utilization=self.config.get('gpu_memory_utilization', 0.9),
                max_model_len=self.config.get('max_model_len', 4096),
                quantization=self.config.get('quantization', None)
            )
            
            # åˆ›å»ºå¼‚æ­¥å¼•æ“
            engine = vllm.AsyncLLMEngine.from_llm(llm)
            
            self.models[model_name] = llm
            self.engines[model_name] = engine
            
            logger.info(f"vLLMæœåŠ¡å¯åŠ¨æˆåŠŸ: {model_name}")
            
        except Exception as e:
            logger.error(f"vLLMæœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
            raise
    
    async def generate(self, 
                      model_name: str, 
                      prompts: List[str], 
                      sampling_params: vllm.SamplingParams) -> List[vllm.RequestOutput]:
        """ç”Ÿæˆæ–‡æœ¬"""
        if model_name not in self.engines:
            raise ValueError(f"æ¨¡å‹æœªåŠ è½½: {model_name}")
        
        engine = self.engines[model_name]
        results = await engine.generate(prompts, sampling_params)
        return results
    
    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        if model_name not in self.models:
            return {}
        
        model = self.models[model_name]
        return {
            'model_name': model_name,
            'model_path': model.model_path,
            'max_model_len': model.max_model_len,
            'tensor_parallel_size': model.tensor_parallel_size,
            'gpu_memory_utilization': model.gpu_memory_utilization
        }
```

#### å¤šæ¥å£é£æ ¼APIæ”¯æŒ
```python
# å¤šæ¥å£é£æ ¼APIé€‚é…å™¨
class MultiInterfaceAPIAdapter:
    def __init__(self, vllm_manager: VLLMServiceManager):
        self.vllm_manager = vllm_manager
    
    # OpenAIé£æ ¼API
    async def openai_chat_completion(self, 
                                   messages: List[Dict[str, str]], 
                                   model: str = "default",
                                   **kwargs) -> Dict[str, Any]:
        """OpenAIé£æ ¼èŠå¤©å®ŒæˆAPI"""
        # æ„å»ºæç¤ºè¯
        prompt = self._messages_to_prompt(messages)
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        sampling_params = vllm.SamplingParams(
            temperature=kwargs.get('temperature', 0.7),
            max_tokens=kwargs.get('max_tokens', 1024),
            top_p=kwargs.get('top_p', 1.0),
            stop=kwargs.get('stop', None)
        )
        
        # ç”Ÿæˆç»“æœ
        results = await self.vllm_manager.generate(model, [prompt], sampling_params)
        
        # è½¬æ¢ä¸ºOpenAIæ ¼å¼
        return self._to_openai_format(results[0])
    
    # RESTful API
    async def rest_generate(self, 
                          prompt: str, 
                          model: str = "default",
                          **kwargs) -> Dict[str, Any]:
        """RESTfulç”ŸæˆAPI"""
        sampling_params = vllm.SamplingParams(
            temperature=kwargs.get('temperature', 0.7),
            max_tokens=kwargs.get('max_tokens', 1024)
        )
        
        results = await self.vllm_manager.generate(model, [prompt], sampling_params)
        
        return {
            'text': results[0].outputs[0].text,
            'model': model,
            'usage': {
                'prompt_tokens': results[0].outputs[0].token_ids[0],
                'completion_tokens': len(results[0].outputs[0].token_ids),
                'total_tokens': len(results[0].outputs[0].token_ids)
            }
        }
    
    # WebSocketæµå¼API
    async def websocket_stream(self, 
                             websocket: WebSocket, 
                             prompt: str, 
                             model: str = "default",
                             **kwargs):
        """WebSocketæµå¼è¾“å‡º"""
        sampling_params = vllm.SamplingParams(
            temperature=kwargs.get('temperature', 0.7),
            max_tokens=kwargs.get('max_tokens', 1024),
            stream=True
        )
        
        async for result in self.vllm_manager.generate_stream(model, [prompt], sampling_params):
            await websocket.send_text(json.dumps({
                'text': result.outputs[0].text,
                'finished': result.finished
            }))
```

### 2. æ¨¡å‹é…ç½®ç®¡ç†

#### æ¨¡å‹é…ç½®æ–‡ä»¶
```yaml
# vllm_models.yaml
models:
  qwen2.5-vl:
    model_path: "models/qwen2.5-vl"
    model_type: "multimodal"
    tensor_parallel_size: 2
    gpu_memory_utilization: 0.9
    max_model_len: 8192
    quantization: "awq"
    
  qwen2.5-7b:
    model_path: "models/qwen2.5-7b"
    model_type: "text"
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.8
    max_model_len: 4096
    quantization: "gptq"
    
  llava-v1.5:
    model_path: "models/llava-v1.5"
    model_type: "multimodal"
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.85
    max_model_len: 4096
    quantization: None

api_interfaces:
  openai:
    enabled: true
    port: 8000
    host: "0.0.0.0"
    
  rest:
    enabled: true
    port: 8001
    host: "0.0.0.0"
    
  websocket:
    enabled: true
    port: 8002
    host: "0.0.0.0"
    
  grpc:
    enabled: false
    port: 8003
    host: "0.0.0.0"
```

### 3. æ€§èƒ½ä¼˜åŒ–

#### å†…å­˜ç®¡ç†
```python
# å†…å­˜ä¼˜åŒ–ç®¡ç†å™¨
class MemoryOptimizer:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.memory_monitor = ResourceMonitor()
    
    def optimize_model_loading(self, model_config: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼˜åŒ–æ¨¡å‹åŠ è½½é…ç½®"""
        current_resources = self.memory_monitor.get_current_resources()
        
        # æ ¹æ®å¯ç”¨GPUå†…å­˜è°ƒæ•´é…ç½®
        available_gpu_memory = current_resources.gpu_memory_available
        
        if available_gpu_memory < 8000:  # å°äº8GB
            model_config['tensor_parallel_size'] = 1
            model_config['gpu_memory_utilization'] = 0.7
            model_config['quantization'] = 'awq'
        elif available_gpu_memory < 16000:  # å°äº16GB
            model_config['tensor_parallel_size'] = 1
            model_config['gpu_memory_utilization'] = 0.8
        else:  # å¤§äº16GB
            model_config['tensor_parallel_size'] = 2
            model_config['gpu_memory_utilization'] = 0.9
        
        return model_config
    
    def get_optimal_batch_size(self, model_name: str) -> int:
        """è·å–æœ€ä¼˜æ‰¹å¤„ç†å¤§å°"""
        current_resources = self.memory_monitor.get_current_resources()
        
        # æ ¹æ®GPUå†…å­˜å’Œæ¨¡å‹å¤§å°è®¡ç®—æœ€ä¼˜æ‰¹å¤„ç†å¤§å°
        gpu_memory = current_resources.gpu_memory_available
        
        if model_name == "qwen2.5-vl":
            if gpu_memory > 16000:
                return 4
            elif gpu_memory > 8000:
                return 2
            else:
                return 1
        else:
            if gpu_memory > 16000:
                return 8
            elif gpu_memory > 8000:
                return 4
            else:
                return 2
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
from src.maoocr.vllm import VLLMServiceManager, MultiInterfaceAPIAdapter

# åˆå§‹åŒ–vLLMæœåŠ¡ç®¡ç†å™¨
vllm_manager = VLLMServiceManager({
    'tensor_parallel_size': 1,
    'gpu_memory_utilization': 0.9,
    'max_model_len': 4096
})

# å¯åŠ¨æ¨¡å‹æœåŠ¡
await vllm_manager.start_service("qwen2.5-vl", "models/qwen2.5-vl")

# åˆ›å»ºAPIé€‚é…å™¨
api_adapter = MultiInterfaceAPIAdapter(vllm_manager)

# ä½¿ç”¨OpenAIé£æ ¼API
response = await api_adapter.openai_chat_completion([
    {"role": "user", "content": "åˆ†æè¿™å¼ å›¾ç‰‡çš„å¤æ‚åº¦"}
], model="qwen2.5-vl", temperature=0.1)

print(response['choices'][0]['message']['content'])
```

### 2. é›†æˆåˆ°MaoOCR

```python
from src.maoocr import MaoOCR
from src.maoocr.vllm import VLLMComplexityAnalyzer

# åˆ›å»ºMaoOCRå®ä¾‹ï¼Œé›†æˆvLLM
ocr = MaoOCR(
    enable_vllm=True,
    vllm_config={
        'model_path': 'models/qwen2.5-vl',
        'tensor_parallel_size': 1
    }
)

# ä½¿ç”¨vLLMè¿›è¡Œå›¾åƒå¤æ‚åº¦åˆ†æ
result = await ocr.recognize_with_vllm_analysis("image.jpg")
print(f"å¤æ‚åº¦åˆ†æ: {result.complexity_analysis}")
print(f"æ¨èç­–ç•¥: {result.recommended_strategy}")
```

### 3. å¤šæ¥å£APIæœåŠ¡

```python
from fastapi import FastAPI, WebSocket
from src.maoocr.vllm import VLLMServiceManager, MultiInterfaceAPIAdapter

app = FastAPI()
vllm_manager = VLLMServiceManager(config)
api_adapter = MultiInterfaceAPIAdapter(vllm_manager)

@app.post("/v1/chat/completions")
async def openai_chat_completion(request: dict):
    """OpenAIé£æ ¼èŠå¤©å®ŒæˆAPI"""
    return await api_adapter.openai_chat_completion(
        request['messages'],
        model=request.get('model', 'default'),
        **request
    )

@app.post("/api/generate")
async def rest_generate(request: dict):
    """RESTfulç”ŸæˆAPI"""
    return await api_adapter.rest_generate(
        request['prompt'],
        model=request.get('model', 'default'),
        **request
    )

@app.websocket("/ws/stream")
async def websocket_stream(websocket: WebSocket):
    """WebSocketæµå¼API"""
    await websocket.accept()
    data = await websocket.receive_json()
    
    await api_adapter.websocket_stream(
        websocket,
        data['prompt'],
        model=data.get('model', 'default'),
        **data
    )
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### vLLM vs å…¶ä»–æ¡†æ¶

| ç‰¹æ€§ | vLLM | Transformers | ONNX Runtime | TensorRT |
|------|------|--------------|--------------|----------|
| **æ¨ç†é€Ÿåº¦** | æœ€å¿« | ä¸­ç­‰ | å¿« | å¿« |
| **å†…å­˜æ•ˆç‡** | æœ€é«˜ | ä¸­ç­‰ | é«˜ | é«˜ |
| **æ‰¹å¤„ç†** | åŠ¨æ€ | é™æ€ | é™æ€ | é™æ€ |
| **æ¨¡å‹æ”¯æŒ** | å¹¿æ³› | å¹¿æ³› | æœ‰é™ | æœ‰é™ |
| **APIæ¥å£** | ä¸°å¯Œ | åŸºç¡€ | åŸºç¡€ | åŸºç¡€ |
| **éƒ¨ç½²å¤æ‚åº¦** | ä½ | ä¸­ç­‰ | é«˜ | é«˜ |

### èµ„æºä½¿ç”¨å¯¹æ¯”

| æ¨¡å‹ | æ¡†æ¶ | GPUå†…å­˜ | æ¨ç†é€Ÿåº¦ | æ‰¹å¤„ç†å¤§å° |
|------|------|---------|----------|------------|
| **Qwen2.5-VL** | vLLM | 8GB | 50ms | 4 |
| **Qwen2.5-VL** | Transformers | 12GB | 100ms | 1 |
| **Qwen2.5-7B** | vLLM | 4GB | 30ms | 8 |
| **Qwen2.5-7B** | Transformers | 8GB | 60ms | 2 |

## ğŸ”® æœªæ¥è§„åˆ’

### 1. æ¨¡å‹ä¼˜åŒ–
- æ”¯æŒæ›´å¤šé‡åŒ–æ ¼å¼ï¼ˆINT4ã€INT8ï¼‰
- ä¼˜åŒ–å¤šæ¨¡æ€æ¨¡å‹æ€§èƒ½
- æ”¯æŒLoRAå¾®è°ƒæ¨¡å‹

### 2. æ¥å£æ‰©å±•
- æ”¯æŒæ›´å¤šAPIé£æ ¼
- å¢åŠ GraphQLæ¥å£
- æ”¯æŒgRPCæµå¼ä¼ è¾“

### 3. éƒ¨ç½²ä¼˜åŒ–
- æ”¯æŒKuberneteséƒ¨ç½²
- å¢åŠ è´Ÿè½½å‡è¡¡
- æ”¯æŒæ¨¡å‹çƒ­æ›´æ–°

### 4. ç›‘æ§å’Œæ—¥å¿—
- å¢åŠ æ€§èƒ½ç›‘æ§
- æ”¯æŒåˆ†å¸ƒå¼è¿½è¸ª
- å¢åŠ æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡

## ğŸ“š æ€»ç»“

vLLMé›†æˆä¸ºMaoOCRæä¾›äº†å¼ºå¤§çš„æœ¬åœ°å¤§æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œé€šè¿‡å¤šæ¥å£é£æ ¼APIæ”¯æŒï¼Œä½¿ç³»ç»Ÿæ›´åŠ çµæ´»å’Œæ˜“ç”¨ã€‚ç»“åˆåŠ¨æ€èµ„æºé€‰æ‹©ç³»ç»Ÿï¼Œèƒ½å¤Ÿæ ¹æ®å½“å‰èµ„æºæƒ…å†µæ™ºèƒ½é€‰æ‹©æœ€ä¼˜çš„æ¨¡å‹å’Œæ¨ç†ç­–ç•¥ï¼Œä¸ºOCRé¢†åŸŸæä¾›äº†ä¸€ä¸ªé«˜æ€§èƒ½ã€æ™ºèƒ½åŒ–çš„è§£å†³æ–¹æ¡ˆã€‚ 